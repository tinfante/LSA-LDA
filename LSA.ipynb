{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                               text\n",
       "0     0  Go until jurong point, crazy.. Available only ...\n",
       "1     0                      Ok lar... Joking wif u oni...\n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3     0  U dun say so early hor... U c already then say...\n",
       "4     0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/spam.csv', encoding='ISO-8859-1', header=0, usecols=[0,1], names=['spam', 'text'])\n",
    "df['spam'] = np.where(df['spam'] == 'ham', 0, 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 9187)\n",
      "747\n",
      "0.13406317300789664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=df.text).toarray()\n",
    "\n",
    "print(tfidf_docs.shape)\n",
    "print(df.spam.sum())\n",
    "print(df.spam.sum()/tfidf_docs.shape[0])\n",
    "\n",
    "# We have 5572 documents, a vocabulary of size 9187, and 747\n",
    "# documents labeled as spam, representing 13% of all docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.201982</td>\n",
       "      <td>0.033972</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>-0.014876</td>\n",
       "      <td>-0.052811</td>\n",
       "      <td>0.038975</td>\n",
       "      <td>-0.042265</td>\n",
       "      <td>-0.034425</td>\n",
       "      <td>0.036801</td>\n",
       "      <td>-0.071311</td>\n",
       "      <td>-0.053138</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.038300</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.054085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.402178</td>\n",
       "      <td>-0.032359</td>\n",
       "      <td>-0.080860</td>\n",
       "      <td>0.077450</td>\n",
       "      <td>0.112747</td>\n",
       "      <td>0.061827</td>\n",
       "      <td>0.031459</td>\n",
       "      <td>0.056353</td>\n",
       "      <td>0.044572</td>\n",
       "      <td>0.020522</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>-0.002010</td>\n",
       "      <td>0.018991</td>\n",
       "      <td>-0.024920</td>\n",
       "      <td>0.056529</td>\n",
       "      <td>0.047418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.030515</td>\n",
       "      <td>0.060799</td>\n",
       "      <td>-0.053171</td>\n",
       "      <td>-0.099954</td>\n",
       "      <td>0.078950</td>\n",
       "      <td>-0.035817</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>-0.035955</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>-0.049890</td>\n",
       "      <td>0.105087</td>\n",
       "      <td>0.030264</td>\n",
       "      <td>-0.030442</td>\n",
       "      <td>-0.020456</td>\n",
       "      <td>-0.053930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.328430</td>\n",
       "      <td>-0.029826</td>\n",
       "      <td>-0.030484</td>\n",
       "      <td>0.009160</td>\n",
       "      <td>0.055054</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>-0.164622</td>\n",
       "      <td>-0.008178</td>\n",
       "      <td>-0.061865</td>\n",
       "      <td>0.089285</td>\n",
       "      <td>-0.077700</td>\n",
       "      <td>-0.023615</td>\n",
       "      <td>0.023060</td>\n",
       "      <td>-0.080385</td>\n",
       "      <td>0.023448</td>\n",
       "      <td>0.035705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.032948</td>\n",
       "      <td>0.032445</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>-0.063115</td>\n",
       "      <td>-0.103995</td>\n",
       "      <td>-0.039177</td>\n",
       "      <td>0.015188</td>\n",
       "      <td>0.060371</td>\n",
       "      <td>-0.052217</td>\n",
       "      <td>-0.006654</td>\n",
       "      <td>0.023747</td>\n",
       "      <td>0.029637</td>\n",
       "      <td>-0.016783</td>\n",
       "      <td>-0.082498</td>\n",
       "      <td>-0.009058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic0    topic1    topic2    topic3    topic4    topic5    topic6  \\\n",
       "0  0.201982  0.033972  0.005732 -0.000226 -0.014876 -0.052811  0.038975   \n",
       "1  0.402178 -0.032359 -0.080860  0.077450  0.112747  0.061827  0.031459   \n",
       "2 -0.030515  0.060799 -0.053171 -0.099954  0.078950 -0.035817  0.003132   \n",
       "3  0.328430 -0.029826 -0.030484  0.009160  0.055054  0.066053 -0.164622   \n",
       "4  0.003097  0.032948  0.032445  0.020450 -0.063115 -0.103995 -0.039177   \n",
       "\n",
       "     topic7    topic8    topic9   topic10   topic11   topic12   topic13  \\\n",
       "0 -0.042265 -0.034425  0.036801 -0.071311 -0.053138 -0.000186 -0.038300   \n",
       "1  0.056353  0.044572  0.020522 -0.010761 -0.002010  0.018991 -0.024920   \n",
       "2 -0.035955  0.012645 -0.072781 -0.049890  0.105087  0.030264 -0.030442   \n",
       "3 -0.008178 -0.061865  0.089285 -0.077700 -0.023615  0.023060 -0.080385   \n",
       "4  0.015188  0.060371 -0.052217 -0.006654  0.023747  0.029637 -0.016783   \n",
       "\n",
       "    topic14   topic15  \n",
       "0 -0.000151 -0.054085  \n",
       "1  0.056529  0.047418  \n",
       "2 -0.020456 -0.053930  \n",
       "3  0.023448  0.035705  \n",
       "4 -0.082498 -0.009058  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have nearly 5000 SMS messages in our dataset and only\n",
    "# 13% of them are labeled as spam. So we have an unbalanced\n",
    "# training set with about 8:1 \"ham\" (normal SMS messages)\n",
    "# to \"spam\" (unwanted solicitations and advertisements).\n",
    "# However, the vocabulary size, is even more problematic.\n",
    "# We have more unique words in our vocabulary than we have\n",
    "# SMS messages. That’s a recipe for over-fitting. So some\n",
    "# dimension reduction/consolidation is definitely in order.\n",
    "# That’s exactly what LSA is for.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=16)\n",
    "pca = pca.fit(tfidf_docs)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs)\n",
    "pca_topic_df = pd.DataFrame(pca_topic_vectors,\n",
    "                                  columns=['topic{}'.format(i) for i in range(16)])\n",
    "pca_topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.955"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use LDA to classify, based on topic vectors, if message is spammy or not.\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(pca_topic_vectors, df.spam, test_size=0.5, random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(xtrain, ytrain)\n",
    "df['pca16_spam'] = lda.predict(pca_topic_vectors)\n",
    "round(float(lda.score(xtest, ytest)), 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
